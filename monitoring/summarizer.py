"""Summarise collections of posts using an LLM with g4f as primary provider, OpenAI as fallback.\n\nimport os\nimport re\nfrom typing import List, Optional\nimport logging\n\n# Configure logging for better error tracking\nlogger = logging.getLogger(__name__)\n\nDEFAULT_HF_MODEL = "facebook/bart-large-cnn"\nMAX_INPUT_LENGTH = 4000\nSUMMARY_MAX_LENGTH = 150\nSUMMARY_MIN_LENGTH = 40\n\ndef summarize(texts: List[str]) -> str:\n    """\n    Return a concise summary of provided texts using available AI models.\n\n    Attempts to use models in order of preference:\n    1. g4f (primary, if configured)\n    2. OpenAI (fallback, if configured)\n    3. Ollama (local AI model) - legacy fallback\n    4. Simple text extraction - lightweight fallback\n\n    Args:\n        texts: List of text strings to summarize\n\n    Returns:\n        A concise summary string\n    """\n    if not texts:\n        return "No content to summarise."\n    # Join texts and limit input length\n    joined = "\n".join(text.strip() for text in texts if text.strip())[:MAX_INPUT_LENGTH]\n    if not joined.strip():\n        return "No meaningful content to summarise."\n\n    # Try g4f first\n    summary = _try_g4f_summary(joined)\n    if summary:\n        return summary\n\n    # Try OpenAI if configured\n    summary = _try_openai_summary(joined)\n    if summary:\n        return summary\n\n    # Try Ollama (legacy)\n    summary = _try_ollama_summary(joined)\n    if summary:\n        return summary\n\n    # Lightweight fallback: simple text extraction\n    logger.info("No AI model available, using simple text extraction")\n    return _simple_fallback(joined)\n\ndef _try_g4f_summary(text: str) -> Optional[str]:\n    """Attempt to summarize using g4f AI provider."""\n    try:\n        import g4f\n        # Example usage; you may want to configure provider/model\n        prompt = (\n            "Summarize the following social media and news content in 2-3 sentences. \n            Focus on the main topics, key events, and overall sentiment. \n            Use bullet points for clarity and bold important phrases.\n            Order your response with the most important points first.\n            Be concise (medium length) and informative:\n            \n            " + text[:2000]\n        )\n        response = g4f.ChatCompletion.create(\n            model="gpt-3.5-turbo",  # or your preferred model\n            messages=[{"role": "user", "content": prompt}]\n        )\n        summary = response.choices[0].message.content.strip() if hasattr(response, "choices") else str(response).strip()\n        if summary and len(summary.split()) > 5:\n            logger.info("Successfully generated summary using g4f")\n            return strip_think(summary)\n        else:\n            logger.warning("g4f returned empty or too short summary")\n    except ImportError:\n        logger.error("g4f package not installed. Install with: pip install g4f")\n    except Exception as exc:\n        logger.error(f"g4f summarization failed: {exc}")\n    return None\n\ndef _try_openai_summary(text: str) -> Optional[str]:\n    """Attempt to summarize using OpenAI API."""\n    try:\n        import openai\n        api_key = os.environ.get("OPENAI_API_KEY")\n        if not api_key:\n            return None\n        openai.api_key = api_key\n        prompt = (\n            "Summarize the following social media and news content in 2-3 sentences. \n            Focus on the main topics, key events, and overall sentiment. \n            Use bullet points for clarity and bold important phrases.\n            Order your response with the most important points first.\n            Be concise (medium length) and informative:\n            \n            " + text[:2000]\n        )\n        response = openai.ChatCompletion.create(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": prompt}]\n        )\n        summary = response.choices[0].message.content.strip() if hasattr(response, "choices") else str(response).strip()\n        if summary and len(summary.split()) > 5:\n            logger.info("Successfully generated summary using OpenAI")\n            return strip_think(summary)\n        else:\n            logger.warning("OpenAI returned empty or too short summary")\n    except ImportError:\n        logger.error("openai package not installed. Install with: pip install openai")\n    except Exception as exc:\n        logger.error(f"OpenAI summarization failed: {exc}")\n    return None\n\ndef _try_ollama_summary(text: str) -> Optional[str]:\n    """Attempt to summarize using Ollama local AI model."""\n    from .secrets import get_secret\n    model = get_secret("OLLAMA_MODEL")\n    ollama_host = get_secret("OLLAMA_HOST", "http://localhost:11434")\n    if not model:\n        return None\n    try:\n        import ollama\n        client = ollama.Client(host=ollama_host)\n        prompt = (\n            "Summarize the following social media and news content in 2-3 sentences. \n            Focus on the main topics, key events, and overall sentiment. \n            Use bullet points for clarity and bold important phrases.\n            Order your response with the most important points first.\n            Be concise (medium length) and informative:\n            \n            " + text[:2000]\n        )\n        response = client.generate(\n            model=model,\n            prompt=prompt,\n            options={\n                "temperature": 0.3,\n                "top_p": 0.9,\n                "max_tokens": 200,\n                "stop": ["---", "\n\n\n"]\n            }\n        )\n        summary = response.get("response", "").strip()\n        if summary and len(summary.split()) > 5:\n            logger.info(f"Successfully generated summary using Ollama model: {model}")\n            return strip_think(summary)\n        else:\n            logger.warning("Ollama returned empty or too short summary")\n    except ImportError:\n        logger.error("ollama package not installed. Install with: pip install ollama")\n    except Exception as exc:\n        logger.error(f"Ollama summarization failed: {exc}")\n        logger.error(f"Check if Ollama is running and model '{model}' is available")\n    return None\n\ndef _simple_fallback(text: str) -> str:\n    """Enhanced fallback: extract key sentences and create a summary."""\n    sentences = [s.strip() for s in text.replace('\n', ' ').split('.') if s.strip()]\n    if not sentences:\n        return strip_think(text[:300] + "...")\n    key_words = ['breaking', 'announced', 'revealed', 'confirmed', 'reported', 'according', 'new', 'first', 'major']\n    summary_sentences = []\n    if sentences:\n        summary_sentences.append(sentences[0])\n    for sentence in sentences[1:-1]:\n        if any(word in sentence.lower() for word in key_words) and len(summary_sentences) < 3:\n            summary_sentences.append(sentence)\n    if len(sentences) > 1 and len(summary_sentences) < 3:\n        summary_sentences.append(sentences[-1])\n    summary = '. '.join(summary_sentences[:3])\n    if not summary.endswith('.'):\n        summary += '.';\n    return strip_think(summary)\n\ndef strip_think(text: str) -> str:\n    """Clean up summary text."""\n    return text.replace("I'm an AI language model", "").strip()\n"""